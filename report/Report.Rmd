---
title: "tidyMC"
subtitle: "An easy-to-use package for Monte-Carlo Simulations"
type: "Report"
author: "Ignacio Moreira Lara, Stefan Linner, Konstantin Lehmann"
discipline: "M.Sc. Econometrics"
date: "`r Sys.Date()`"
supervisor: "Jens Klenke"
secondsupervisor: "Martin C. Arnold"
studid: 230658, 233565, 229994
cols_authors: 4
estdegree_emester: "Summer Term 2022"
deadline: "06.09.2022"
output:
  pdf_document:
    extra_dependencies: ["lmodern", "mathtools", "amsmath", "amsfonts", "soul"]
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: false
toc: false
lot: false
lof: false
graphics: true
biblio-title: References
fontsize: 10pt
geometry: lmargin=2.5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references_tidy.bib
classoption: a4paper
language: english
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library(profvis)
# library(purrr)
# library(dplyr)
# library(tibble)
# library(rlang)
# library(ggplot2)
devtools::load_all()
```

# Introduction
MSC is statistical technique to analyze statistical techniques. 


# Monte-Carlo Simulation
Monte Carlo Simulations (henceforth MCS) allow analyzing the properties of classic econometric inference techniques. As a simulation-based technique, MCS applies a statistical procedure to many synthetically created samples. The repeated application of the procedure to a fully known GDP helps to investigate properties of statistical techniques in two situations. For one, it allows to scrutinize the properties of asymptotically consistent estimators in smaller samples. For another, it helps to analyze properties in situations which are analytically not solvable. In this section we want to give a brief overview how and in which circumstances work, mainly following.
<!-- \citep{kiviet_monte_2011}. -->


## Pseudo Random Number Generation and the Data Generating Process
MCS generally starts with a synthetically created i.i.d. sample. Computers are, however, not able to generate truly random draws from different distributions. Instead the computer's draws from different distributions are generated through pseudo-random draws from a uniform distribution. While we do not provide a detailed description of the different algorithms used, we explain the underlying intuition behind random number generation. 

Creating pseudo-random numbers from a probability distribution generally requires creating pseudo-random draws from a standard uniform distribution. Draws from a uniform distribution are created through the application of a certain algorithm, called random number generators (RNG), to a natural number, called the seed. The created values are pseudo-random because they are indistinguishable from truly random numbers but are fully determined by the combination of the algorithm and the seed. Combined knowledge of the algorithm and the seed allows exact replication of a series of pseudo-random numbers generated. The possibility to replicate the draws allows to exactly reproduce results that build on the pseudo-randomly generated number on different computers. 

The most common algorithm, as well as R's standard algorithm, used for the generation of pseudo-random draws from a standard uniform distribution, is the Mersenne-Twister algorithm. There are however more algorithms available. In R it is also possible to specify your own algorithm to create pseudo-random numbers.  The purpose of the tidyMC-package is to additionally allow easy parallelization od the MCS. With parallel computing, special attention has to be payed to create disjoint and sufficiently long sets of draws from a uniform distribution. Our package builds on R-standard Lâ€™Ecuyer-CMRG RNG streams. The algorithm creates reproducible sets of $U[0,1]$ numbers for each parallel worker, using an integer seed of length 7. Alternatively a seed of length 1 is accepted, which corresponds to a valid seed of length 7. Because the algorithm creates a subset of random numbers for each workers, the stream and hence the MCS results are only reproducible if the same number of parallel workers is used. Alternatively our function allows to supply user-generated lists of random variables. 

Generating pseudo-random numbers from more complex distributions uses the distribution's cumulative distribution function (CDF). It can be shown that every random variable $X$ has a CDF $F$ if $X = F^{-1}(U)$, where U is a random variable uniformly distributed on $(0,1)$. Pseudo-random draws from different probability distributions are conceptually implemented by applying the inverse of CDF to pseudo-random numbers from the uniform distribution. Using the CDF works directly for continuous random variables. If the random variable is discrete, the CDF is a discrete step function. In that case ... .Direct application of the inverse of CDF is however often computationally time-consuming and requires that the inverse CDF is analytically available. To account for the two shortcomings different approaches have been developed for specific distributions, that also easily extend to random vectors. While they build on the idea to use the inverse of the CDF, their concrete design is out of the scope of this paper. A general overview can be found in the documentation of the respective R-functions. 

Complex multivariate distribution can then be simulated as functions of standard distributions. A typical example of that is a linear conditional expectation functions, the work horse model of econometrics. In the following example.who creation of uniform numbers. 

## MCS moment estimation
MCS allows to approximate the distribution of a statistic for prespecified DGP. MCS results for a statistic hence hold generally only for the prespecified DGP. Because it additionally only approximates the distribution numerically, we need to control for the resulting approximation inaccuracies.  To provide a general understanding of these two limitations, we next provide a outline of MCS Moment Estimations 

### MCS set-up

In MCs we generally simulate the distribution of a statistic $q_n$. The statistic can often be expressed as a function of a parameter vector $\theta$, some deterministic variables $D$ and some random variables $v$, s.t. $q_n = q_n(\theta,D,n)$. 

Sho here pag 20. how then MC works and why consistent for functions of means



<!-- For example we might be interested in the ability of OLS to estimate the variable parameter $\beta_1$ of a simple linear regression model with deterministic regressor. -->
<!-- In that case specify the model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ with corresponding true population paramterers $\theta  = (\beta_0, \beta_1)$, $D = X$ a $n \times 2$ matrix of the deterministic x and the intercept $\beta_0$, and $c = \epsilon$ a $n \times 1$ of $\mathcal{N}(0, \sigma_\epsilon)$ as a normally distributed error term. Using OLS, we estimate $\hat{\beta_1}$ as the second element of $(X^TX)^{-1}X^Ty$. With OLS then $\hat{\beta}_1(\theta, D, v)$. In a MCS we would then generate r $\hat{\beta_1}$ to study how well OLS is able to capture the true $\beta_1$.  -->
<!-- Show distribution of interval  -->

# Monte Carlo example

## OLS consistency

One classic example for Monte Carlo simulations is the estimation of linear models by ordinary least squares (OLS). Given a dependent variable $y$ and a set of iid. regressors $X = \{x_1, x_2\}$ we consider a DGP of the following structure:
\begin{equation}
	\label{eq_DGP}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon,
\end{equation}  
where $\beta  = \{\beta_0, \beta_1, \beta_2\}$ are the coefficients and $\varepsilon$ is a normally distributed error term with variance $\sigma^2$, i.e. $\varepsilon \sim \mathcal{N}(0,\sigma^2)$. Using this GDP process random samples containing observations $i$ of size $n$ are generated, then by the use of OLS we obtain the estimators $\hat{beta}$ and $s^2$ for $\beta$ and $\sigma^2$, respectively. Writing the regressors in matrix notation, and minimizing the sum of squared errors, we obtain the following estimators:
\begin{align}
	\hat{\beta} = (XX)^{-1} XY,\\
	s^2 =   \frac{1}{n-k} \hat{\varepsilon}' \hat{\varepsilon},
\end{align}
where $k$ is the number of regressors and $\hat{\varepsilon}$ is the vector of residuals obtained from the regression. Both estimators are consistent and unbiased, therefore their precision should improve with an increasing sample size.

\noindent Using Monte Carlo simulations we can easily test this. Let $\beta = \{1, 4, 5\}$ and $\sigma^2 = 3$, then we generate random draws for $x_1$ and $x_2$ from normal distributions with different means and variances. Additionally, we generate $\varepsilon$ from its respective distribution. Now the parameter of interest for our function is the number of observations we use for our estimation, for this we set $n = \{100, 200, 300\}$. Lastly, we repeat this process 10000 times for each parameter combination. We present the results in table \ref{tab:ols_table}, where the first column is related to the number of observations we use in the estimation. Moreover the last 4 columns show the results' mean over a number or repetitions for each parameter combination. Namely, in the first panel the mean over the estimated coefficients from the 10 first repetitions is calculated; the subsequent panel shows the mean calculated using all Monte Carlo repetitions. From the results, we empirically prove the performance of OLS estimation with increasing sample sizes, i.e. using a bigger sample size leads to a more precise estimation of all underlying population values. Comparing both panels, we see that this behavior is just confirmed, as the estimated values do not differ and they become more precise.

```{r, echo = FALSE, eval = TRUE, results = "hide", message=FALSE, fig.show='hide'}
ols_test <-
  function(b0, b1, b2, n, sigma2, param_x1, param_x2, inc_x2){

    # generation of data
    x1 <- rnorm(n = n, mean = param_x1[1], sd = param_x1[2])
    x2 <- rnorm(n = n,  mean = param_x2[1], sd = param_x2[2])
    e <- rnorm(n, sd = sqrt(sigma2))
    y <- b0 + b1*x1 + b2*x2 + e

    if (inc_x2 == 0){
      x2 <- x2 * inc_x2
    }

    # application of method
    estim <- lm(y ~ x1 + x2)

    # evaluation of the result for a single repetition and parameter combination
    out <- list(B0 = estim$coefficients[1],
                B1 = estim$coefficients[2],
                B2 = estim$coefficients[3],
                s2 = var(estim$residuals))
    return(out)
  }


param_list_ols <-
  list(n = c(100, 200, 300))

ols <- future_mc(fun = ols_test, repetitions = 10000, param_list = param_list_ols,
                 b0 = 1, b1 = 4, b2 = 5, param_x1 = c(1,2), param_x2 = c(3,4),
                 sigma2 = 3, inc_x2 = 1)
invisible(ols_plots <- plot(ols))
```

```{r ols_table, echo = FALSE}

tidy_mc_latex(summary(ols), repetitions_set = c(10, 10000),
                           column_names = c("$\\beta_0$", "$\\beta_1$",
                                            "$\\beta_2$", "$s^2$"))

```
Similarly to show the convergence of the estimators in a graphical manner, we present histograms of the estimated coefficients for all values of $n$. We present the histograms for $\hat{\beta_1}$ and $s^2$ in figures \ref{fig:ols_b1} and \hl{(XX)}, and include the remaining plots in the Appendix. The pattern for both figures is clear, the increase in $n$ reduces the variance of the estimator around the true value, thus the consistency of OLS is empirically visible.

```{r ols_b1, echo = FALSE, fig.cap="MC $\\beta_1$ results"}
ols_plots$B1

```

```{r ols_s2, echo = FALSE, fig.cap="MC $s^2$ results"}
ols_plots$s2

```



##Ommited variable bias and irrelevant regressor inclusion

Moreover, another common problem for estimation is the inclusion of suitable regressors into the model. Both the inclusion irrelevant variables and the absence of relevant ones relevant ones into the regression model cause problems when estimating the true parameters. In this section, we will test the impact both of these problems on the estimated values.
\noindent We continue with the same structure as in equation \eqref{eq_DGP}, however we will test the case when $\beta_2 = 0$ which means that $x_2$ has no influence over $y$. This causes a problem in the estimation when the researcher does not know the underlying DGP and includes $x_2$ to the model, because of lack of information or convenience. \textit{A priori} the expected result is that the estimated coefficient associated with this new variable will be 0, while the variance of the residuals however will increase.

\noindent We test this hypothesis using a Monte Carlo study, where we use the same coefficient values as for the base example, except that we set $\beta_2=0$. Then we run an OLS model with $x_1$ and $x_2$ as the regressors using $n = (100, 200, 300)$. Lastly, we repeat this 10000 times for every value of $n$. We present the obtained results in table \ref{eq:irr_ols_table}. Two panels are presented again, one for the mean results of the ten first MC repetitions and the second for all repetitions. Again the hypothesis about the behavior of the coefficients is proven to be right, i.e. the estimators for $\beta_0$ and $\beta_1$ appear to converge to the true value. In contrast, the estimator for $\beta_2$ is marginally different from zero for all values of $n$, and from the overall mean results we conclude that this behavior does not change. Lastly, the last point of the hypothesis about the estimator of the variance of the error term is also proven to be right. This is because the results for the estimator $s^2$ have higher values compared to the ones obtained in table \ref{ols_table}. 

```{r, echo = FALSE, eval = TRUE, results = "hide", message=FALSE}
ols_irr <- future_mc(fun = ols_test, repetitions = 10000,
                     param_list = param_list_ols, b0 = 1, b1 = 4,
                     b2 = 0, param_x1 = c(1,2), param_x2 = c(3,4),
                     sigma2 = 3, inc_x2 = 1)

```


```{r irr_ols_table, echo = FALSE}
tidy_mc_latex(summary(ols_irr), repetitions_set = c(10, 10000),
                               column_names = c("$\\beta_0$", "$\\beta_1$",
                                                "$\\beta_2$", "$s^2$"),
              caption = "MC OLS results with an irrelevant variable")

```

Lastly, we test the effects of estimating a model using only $x_1$ when the underlying DGP follows the structure of \eqref{eq_DGP}, this problem is most commonly known as having an omitted variable bias. The theoretical consequences of this problem are related to the consistency and unbiasedness of the estimators. Since it is necessary that $x_1$ and $x_2$ share a degree of correlation between them, the absence of one of them in the regression model will cause the available variable to be correlated with the error term, thus violating one of the main assumptions of this methodology. This correlation will lead to an estimator of $\beta_1$ to have a higher or lower expected value (depending on the sign of the correlation) in comparison to the true parameter.
















# Strucutre
-Introduction to MCS (5)
- How it works in our package (10)
  - parallelisation plan
  - function
  - return objects
    - objects
    - summary
    - plots
    - tables 
  - exampl of outputs
  - extension: bootstrap
- Comparison to Monte Carlos package (3)
  - objects
  - speed
    -bench::mark
- Vignette

\end{document}

