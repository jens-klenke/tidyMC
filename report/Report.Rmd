---
title: "tidyMC"
subtitle: "An easy-to-use package for Monte-Carlo Simulations"
type: "Report"
author: "Ignacio Moreira Lara, Stefan Linner, Konstantin Lehmann"
discipline: "M.Sc. Econometrics"
date: "`r Sys.Date()`"
supervisor: "Jens Klenke"
secondsupervisor: "Martin C. Arnold"
studid: 230658, 233565, 229994
cols_authors: 4
estdegree_emester: "Summer Term 2022"
deadline: "06.09.2022"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: false
toc: false
lot: false
lof: false
graphics: true
biblio-title: References
fontsize: 10pt
geometry: lmargin=2.5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
language: english
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(profvis)
library(purrr)
library(dplyr)
library(tibble)
library(rlang)
library(ggplot2)
```

# Introduction
MSC is statistical technique to analyze statistical techniques. 

Rephrase and Add!
MCS results lack generality,
are nonanalytical but numerical, and are random as well Therefore,
they are both very specific and involve inaccuracies, and only after fully
understanding the nature and magnitude of these inaccuracies we will
move on and apply MCS in a range of situations where the true underlying
properties of estimators and tests are mostly unknown and the
estimated numerical results from MCS establish, next to their analytic
asymptotic approximations, our only guidebook.


# Monte-Carlo Simulation
Monte Carlo Simulations (henceforth MCS) allow analyzing the properties of classic econometric inference techniques. As a simulation-based technique, MCS applies a statistical procedure to many synthetically created samples. By applying a statistical technique to a fully known GDP, MCS helps to investigate properties of statistical techniques in two situations. For one, it allows to scrutinize the properties of asympotically consistent estimators in smaller samples. For another, it helps to analyze properties in situations which are analytically not solvable. In this section we want to give a brief overview how and in which circumstances work, mainly following \citep{kiviet_monte_2011}. 


## Pseudo Random Number Generation and the Data Generating Process
MCS generally startes with a synthetically created i.i.d. sample. Computers are however not able to generate truly random draws from different distributions. Instead the computer's draws from different distributions are generated through pseudo-random draws from a uniform distribution. While we do not provide a detailed description of the different algorithms used, we explain the underlying intuition behind random number generation. 

Creating pseudo-random numbers from a probability distribution generally requires creating pseudo-random draws from a uniform distribution. Draws from a uniform distribution are created through the application of a certain algorithm, called random number generators (RNG), to a natural number, called the seed. The created values are pseudo-random because they are indistinguishable from truly random numbers but are fully determined by the combination of the algorithm and the seed. Combined knowledge of the algorithm and the seed allows exact replication of a series of pseudo-random numbers generated. The possibility to replicate the draws allows to exactly reproduce results that build on the pseudo-randomly generated number on different computers. 

The most common algorithm, as well as R's standard algorithm, used for the generation of pseudo-random draws from a  uniform (0,1) distribution, is the Mersenne-Twister algorithm. There are however more algorithms available. In R it is also possible to specify your own algorithm to create pseudo-random numbers.  The purpose of the tidyMC-package is to additionally allow easy parallelization. With parallel computing, special attention has to be payed to create disjoint and sufficiently long sets of draws from a unifrom distrbution. Our package builds on R-standard Lâ€™Ecuyer-CMRG RNG streams. The algorithm creates reproducible sets of $U[0,1]$ numbers for each parallel worker, using an integer seed of length 7. Alternatively a seed of length is accepted, which corresponds to a seed of length 7. Because the algorithm creates a subset of random numbers for each workers, the stream and hence the MCS results are only reproducible if the same number of parallel workers is used. Alternatively our function allows to supply user-generated lists of random variables. 

Generating pseudo-random numbers from more complex distributions uses the distribtion's cumulative distribution function (CDF). It can be shown that every random variable $X$ has a CDF $F$ if $X = F^{-1}(U)$, where U is a random variable uniformly distributed on $(0,1)$. Pseudo-random draws from different probability distributions are conceptually implemented by applying the inverse of CDF to pseudo-random numbers from the uniform distribution. Using the CDF works directly for continuous random variables. If the random variable is discrete, the CDF is a discrete step function. In that case ... .Direct application of the inverse of CDF is however often computationally rather time-consuming and requires that the inverse CDF is analytically available. To account for the two shortcomings different approaches have been developed for specific distributions, that also easily extend to random vectors. While they build on the idea to use the inverse of the CDF, their concrete design is out of the scope of this paper. A general overview can be found in Simulations chapter. 

Complex multivariate distrbution can then be simulated as functions of standard distributions. A typical example of that is a linear conditional expectation functions, the work horse model of econometrics. In the following example... Add examle with CLT and LLN as on page 20 of pdf!

## MCS moment estimation
MCS allows to approximate the distrbution of a statistic for persepcified DGP. MCS results for a statistic hence hold generally only for the prespecified DGP. Because it additionally only approximates the distrbution numerically, we need to control for the resultung approximation inaccurarcies.  To provide a general understanding of these two limitations, we next provide a outline of MCS Moment Estimations 

### MCS set-up

In MCs we generally simulate the distrbution of a statistic $q_n$. The statistic can often be expressed as a function of a parameter vector $\theta$, some deterministic variables $D$ and some random variables $v$, s.t. $q_n = q_n(\theta,D,n)$. 


For example we might be interested in the ability of OLS to estimate the variable parameter $\beta_1$ of a simple linear regression model with deterministic regressor.
In that case specify the model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ with corresponding true population paramterers $\theta  = (\beta_0, \beta_1)$, $D = X$ a $n \times 2$ matrix of the deterministic x and the intercept $\beta_0$, and $c = \epsilon$ a $n \times 1$ of $\mathcal{N}(0, \sigma_\epsilon)$ as a normally distributed error term. Using OLS, we estimate $\hat{\beta_1}$ as the second element of $(X^TX)^{-1}X^Ty$. With OLS then $\hat{\beta}_1(\theta, D, v)$. In a MCS we would then generate r $\hat{\beta_1}$ to study how well OLS is able to capture the true $\beta_1$. 




\end{document}

